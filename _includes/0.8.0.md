The first [alpha version](https://github.com/optimizationBenchmarking/optimizationBenchmarking/releases/tag/v0.8.0) of the *optimizationBenchmarking.org* evaluator has been released, together with a first version of the project introduction slides and two examples.

This version is already functional although it still lacks some major features and *may* be buggy. The generated reports are still empty except for the figures, several features of the [TSP Suite](https://github.com/optimizationBenchmarking/tspSuite) evaluator are missing and will be implemented step-by-step into this more general evaluator component.

You can already use it
to evaluate data gathered from your own experiments,
with [COCO](http://coco.gforge.inria.fr/doku.php?id=start)),
or with [TSP Suite](https://github.com/optimizationBenchmarking/tspSuite). It can
produce reports in LaTeX, XHTML, or in a textual format suitable for export to
other applications. So, although we still have some way to go, this release is a
first major milestone and can already be used. 