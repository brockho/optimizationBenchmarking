---
layout: post
title:  "Second Alpha Release: Version 0.8.1"
date:   2015-06-28
categories: page
---

Less than one month after the initial release of the first [alpha version](https://github.com/optimizationBenchmarking/optimizationBenchmarking/releases/tag/v0.8.0),
we are happy to announce the second [alpha version](https://github.com/optimizationBenchmarking/optimizationBenchmarking/releases/tag/v0.8.1)
of our *optimizationBenchmarking.org* evaluator.

You can download it from our [Maven repository]({{ site.baseurl }}/repo/optimizationBenchmarking/org/optimizationBenchmarking/0.8.1/index.html), which includes a stand-alone binary, or from [GitHub](https://github.com/optimizationBenchmarking/optimizationBenchmarking/releases/tag/v0.8.1), where only the sources are provided.

The new release has some exciting new functionality (this is not hipster-buzzword-speak, I am truly a bit excited that this works):

1. You can now transform input data for the supported diagrams with almost arbitrary
   functions. Let's say you want the x-axis of an ECDF diagram to not just represent
   your function evaluations (`FEs`) dimensions, but the logarithm of `1+FEs`? Then
   just specify `<cfg:parameter name="xAxis" value="lg(1+FEs)" />` in your
   `evaluation.xml`!
2. You can use all numerical instance features and experiment parameters, as well
   as dimension limits in these expressions too! Let's say you have measured data
   for experiments with Traveling Salesman Problems (TSPs) and you also want to
   scale the above FEs by dividing them by the square of the feature `n` which
   denotes the number of cities in a benchmark instance. Then how about specifying
   `<cfg:parameter name="xAxis" value="lg((1+FEs)/(n²))" />`?
3. Under the hood, the font support has been improved. When creating LaTeX output,
   we use the same fonts as LaTeX uses. However, these may not have glyphs for some
   unicode characters -- maybe they (e.g., `cmr`) do not support `²`. In order
   to deal with this, we use composite fonts which then render `²` with a glyph from
   a platform-default font which has that gylph. Not beautiful, but for now it will
   do.
4. We now actually print some form of descriptive text into our reports. There still
   is quite some way to go to get good text, but we are moving forward.

Although we are still not near our vision of automated interpretation of experimental
results in the domains of Machine Learning and optimization,
[release 0.8.1]({{ site.baseurl }}/repo/optimizationBenchmarking/org/optimizationBenchmarking/0.8.1/index.html)
marks a step forward. It allows you to explore your experiment data much more freely
and hopefully to find interesting relationships between your algorithm's performance,
its parameters, and the features of your benchmark instances.

The [slides]({{ site.data.documentation.evaluatorSlidesURL }}) describing the evaluator
together with some examples have been updated as well.